# =============================================================================
# HDFS 数据稽核配置文件
# =============================================================================
# 本文件定义了调度任务与 Hive 表的映射关系，以及表的分区和格式信息

# -----------------------------------------------------------------------------
# 全局默认配置
# -----------------------------------------------------------------------------
defaults:
  # 默认数据日期
  # 支持: ${yesterday}, ${today}, 或具体日期如 20260115
  data_date: "${yesterday}"
  
  # Python 程序并发控制
  # 同时稽核几个表（并发调用多个 hdfs-counter.jar 进程）
  # 设为 1 表示串行执行
  python_concurrency: 5
  
  # 传递给 hdfs-counter.jar 的默认参数
  jar_options:
    threads: 10                  # 单个jar内部并发读取文件的线程数
  
  # 安全限流（建议生产环境开启）
  # 说明：外层 Python 并发 * 内层 jar threads = “有效并发倍率”
  # 为避免把机器/集群打爆，程序会自动对超限值进行 clamp，并输出 warning 日志。
  limits:
    max_python_concurrency: 20        # Python 同时跑多少个表（多少个 jar 进程）
    max_jar_threads: 50               # 单个 jar 内最大线程数
    max_effective_parallelism: 200    # python_concurrency * jar_threads 的上限

# -----------------------------------------------------------------------------
# 调度任务与表的映射配置
# -----------------------------------------------------------------------------
# 每个 schedule 代表一个调度任务，包含该任务产出的一个或多个表
# 当调度任务完成后，系统会自动对这些表进行数据量稽核

schedules:
  # -------------------------------------------------------------------------
  # 示例1: 用户行为分析任务 - 单表单分区
  # -------------------------------------------------------------------------
  - task_name: "dw_user_daily"
    interface_id: "1001" # 接口ID
    partner_id: "1" # 合作伙伴id
    tables:
      - name: "dw.user_behavior"
        hdfs_path: "/warehouse/dw.db/user_behavior"
        format: "orc"
        partition_template: "dt=${data_date}"
        province_id: "1" #省份id  
  
  # -------------------------------------------------------------------------
  # 示例2: 订单数据任务 - 单任务多表
  # -------------------------------------------------------------------------
  - task_name: "dw_order_daily"
    interface_id: "1002"
    partner_id: "2"
    tables:
      - name: "dw.order_fact"
        hdfs_path: "/warehouse/dw.db/order_fact"
        format: "parquet"
        partition_template: "dt=${data_date}"
      
      - name: "dw.order_detail"
        hdfs_path: "/warehouse/dw.db/order_detail"
        format: "orc"
        partition_template: "dt=${data_date}"
        # 可以覆盖全局线程数配置
        threads: 20
  
  # -------------------------------------------------------------------------
  # 示例3: 日志导入任务 - TextFile格式，多级分区但只统计到日期级别
  # -------------------------------------------------------------------------
  # 说明: 实际分区结构为 dt=20260101/hour=00 ~ hour=23
  #       配置 partition_template: "dt=${data_date}" 会递归统计所有子分区
  - task_name: "ods_log_import"
    interface_id: "1003"
    partner_id: "3"
    tables: 
      - name: "ods.access_log"
        hdfs_path: "/warehouse/ods.db/access_log"
        format: "textfile"
        delimiter: "\n"          # 行分隔符，默认为换行符
        partition_template: "dt=${data_date}"
  
  # -------------------------------------------------------------------------
  # 示例4: 区域数据任务 - 多级分区，只统计指定子分区
  # -------------------------------------------------------------------------
  - task_name: "ods_region_data"
    interface_id: "1004"
    partner_id: "4"
    tables:
      - name: "ods.region_sales_japan"
        hdfs_path: "/warehouse/ods.db/region_sales"
        format: "orc"
        # 只统计日本区域的数据
        partition_template: "dt=${data_date}/dp=Japan"
      
      - name: "ods.region_sales_usa"
        hdfs_path: "/warehouse/ods.db/region_sales"
        format: "orc"
        # 只统计美国区域的数据
        partition_template: "dt=${data_date}/dp=USA"

# =============================================================================
# 配置说明
# =============================================================================
# 
# defaults 部分:
#   data_date:        默认数据日期，支持 ${yesterday}, ${today} 或具体日期
#   python_concurrency: Python 程序并发数，同时稽核几个表（默认1，串行执行）
#   jar_options.threads: 单个 jar 内部读取文件的并发线程数
#
# task_name:          调度平台上的任务名称，用于与完成的任务进行匹配
# interface_id:       接口ID
# partner_id:         合作伙伴id
# tables:             该任务产出的表列表
#   - name:           Hive 表的完整名称 (database.table)
#   - hdfs_path:      表在 HDFS 上的基础路径（不含分区）
#   - format:         文件格式，支持: orc, parquet, textfile
#   - partition_template: 分区模板，支持变量 ${data_date}
#   - delimiter:      (可选) textfile 的行分隔符，默认为 \n
#   - threads:        (可选) 覆盖全局 jar_options.threads 配置
#   - province_id:    (可选) 省份id
#
# 变量说明:
#   ${data_date}      数据日期，格式为 YYYYMMDD
#   ${yesterday}      昨天的日期
#   ${today}          今天的日期
# =============================================================================

